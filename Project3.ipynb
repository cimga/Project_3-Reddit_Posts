{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split,cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "client_auth = requests.auth.HTTPBasicAuth('PMd9sKprwheUIQ', 'k7MAvz4NDVQKOgfqv41Rqf3FCUo')\n",
    "post_data = {\"grant_type\": \"password\", \"username\": \"cimga\", \"password\": \"cimga5823494\"}\n",
    "headers = {\"User-Agent\": \"ChangeMeClient/0.1 by cimga\"}\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data,\n",
    "                         headers=headers)\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": \"bearer 259091767874-Ivv2hkMm_dmvMcCLrSGxSmFLKfk\", \"User-Agent\": \"ChangeMeClient/0.1 by cimga\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='PMd9sKprwheUIQ',\n",
    "                     client_secret='k7MAvz4NDVQKOgfqv41Rqf3FCUo',\n",
    "                     user_agent='ChangeMeClient/0.1 by cimga',\n",
    "                    \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in reddit.subreddit('travel').hot(limit=3000):\n",
    "    l.append([i.id,i.subreddit,i.title,i.selftext])\n",
    "    \n",
    "for i in reddit.subreddit('geography').hot(limit=3000):\n",
    "    l.append([i.id,i.subreddit,i.title,i.selftext])\n",
    "    \n",
    "l[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns=['id','subreddit','title','selftext']\n",
    "df=pd.DataFrame(l,columns=columns)\n",
    "df[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./first_data',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./first_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1968, 5)"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b9bzar</td>\n",
       "      <td>travel</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices'</td>\n",
       "      <td>Hey travellers!\\n \\nIn this weekly community d...</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b9u8w8</td>\n",
       "      <td>travel</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id subreddit                                              title  \\\n",
       "0  b9bzar    travel    r/travel Topic of the Week: 'Lifestyle Choices'   \n",
       "1  b9u8w8    travel  Just outside Marrakesh, Morocco (those are res...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hey travellers!\\n \\nIn this weekly community d...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                              merged  \n",
       "0  r/travel Topic of the Week: 'Lifestyle Choices...  \n",
       "1  Just outside Marrakesh, Morocco (those are res...  "
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "subreddit      0\n",
       "title          0\n",
       "selftext     905\n",
       "merged         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b9bzar</td>\n",
       "      <td>travel</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices'</td>\n",
       "      <td>Hey travellers!\\n \\nIn this weekly community d...</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b9u8w8</td>\n",
       "      <td>travel</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b9onlo</td>\n",
       "      <td>travel</td>\n",
       "      <td>Angels Landing- Zion National Park</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Angels Landing- Zion National Park unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id subreddit                                              title  \\\n",
       "0  b9bzar    travel    r/travel Topic of the Week: 'Lifestyle Choices'   \n",
       "1  b9u8w8    travel  Just outside Marrakesh, Morocco (those are res...   \n",
       "2  b9onlo    travel                 Angels Landing- Zion National Park   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hey travellers!\\n \\nIn this weekly community d...   \n",
       "1                                            unknown   \n",
       "2                                            unknown   \n",
       "\n",
       "                                              merged  \n",
       "0  r/travel Topic of the Week: 'Lifestyle Choices...  \n",
       "1  Just outside Marrakesh, Morocco (those are res...  \n",
       "2         Angels Landing- Zion National Park unknown  "
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna('unknown',inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>geography</th>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  title  selftext  merged\n",
       "subreddit                              \n",
       "geography  720    720       720     720\n",
       "travel     185    185       185     185"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['selftext']=='unknown'].groupby('subreddit').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7955801104972375"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "720/905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to binary our y value\n",
    "df['subreddit']=df['subreddit'].replace({'travel':1,'geography':0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b9bzar</td>\n",
       "      <td>1</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices'</td>\n",
       "      <td>Hey travellers!\\n \\nIn this weekly community d...</td>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b9u8w8</td>\n",
       "      <td>1</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b9onlo</td>\n",
       "      <td>1</td>\n",
       "      <td>Angels Landing- Zion National Park</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Angels Landing- Zion National Park unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  subreddit                                              title  \\\n",
       "0  b9bzar          1    r/travel Topic of the Week: 'Lifestyle Choices'   \n",
       "1  b9u8w8          1  Just outside Marrakesh, Morocco (those are res...   \n",
       "2  b9onlo          1                 Angels Landing- Zion National Park   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  Hey travellers!\\n \\nIn this weekly community d...   \n",
       "1                                            unknown   \n",
       "2                                            unknown   \n",
       "\n",
       "                                              merged  \n",
       "0  r/travel Topic of the Week: 'Lifestyle Choices...  \n",
       "1  Just outside Marrakesh, Morocco (those are res...  \n",
       "2         Angels Landing- Zion National Park unknown  "
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['merged']=df['title'] +' '+ df['selftext']\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['merged'] # in series form NOT dataframe!\n",
    "y=df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs.best score(cros_val_score): 0.9165402124430956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vec__max_features': 5000,\n",
       " 'vec__ngram_range': (1, 2),\n",
       " 'vec__stop_words': 'english'}"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe=Pipeline( [ ('vec',CountVectorizer()),\n",
    "                  ('model',MultinomialNB())\n",
    "               ])\n",
    "\n",
    "pipe_params={ \n",
    "    'vec__stop_words': [None, 'english'],\n",
    "    'vec__max_features': [30,250,300,1000,5000,None],\n",
    "    'vec__ngram_range': [(1,1), (1,2)]\n",
    "    \n",
    "#     'vec__token_pattern':['(?u)\\\\b\\\\w\\\\w+\\\\b']\n",
    "#     prefixes must be the same with above mentioned \n",
    "#     vector and model. otherwise it deosnt recognize.\n",
    "#         'stop_wrods':[None,'english'],\n",
    "#         'max_features':[2000],\n",
    "#         'n_gram':[(1,1),(1,2)]\n",
    "                }\n",
    "\n",
    "gs=GridSearchCV(pipe,param_grid=pipe_params,cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('gs.best score(cros_val_score):',gs.best_score_)\n",
    "feeder=pipe.named_steps\n",
    "best_paramaters=gs.best_params_\n",
    "best_paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this code supposed to work but didnt! maybe need to update the sklearn\n",
    "#pd.DataFrame(gs.cv_results_).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split for Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/travel Topic of the Week: 'Lifestyle Choices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just outside Marrakesh, Morocco (those are res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angels Landing- Zion National Park unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The ancient village of Misfat Al Abriyeen in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fishing boat in Ha Long Bay at dawn. Pearl oys...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              merged\n",
       "0  r/travel Topic of the Week: 'Lifestyle Choices...\n",
       "1  Just outside Marrakesh, Morocco (those are res...\n",
       "2         Angels Landing- Zion National Park unknown\n",
       "3  The ancient village of Misfat Al Abriyeen in t...\n",
       "4  Fishing boat in Ha Long Bay at dawn. Pearl oys..."
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df[['merged']]  # dataframe format\n",
    "y=df['subreddit']\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.505589\n",
       "0    0.494411\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline is % 51\n",
    "y.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.33, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- BAYES CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a-CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search Output:\n",
    "- {'vec__max_features': 5000,\n",
    " 'vec__ngram_range': (1, 2),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we use the __grid search__ output: max_feature=2500, stop_words=None, ngram_range=(1, 1)\n",
    "- cross val score: 0.902\n",
    "- train score    : 0.943\n",
    "- test score     : __0.430__\n",
    "\n",
    "If we enter parameters __intuitively__ : max_feature=1500, stop_words=None, ngram_range=(1, 1)\n",
    "- cross val score: 0.897\n",
    "- train score    : 0.927\n",
    "- test score     : __0.620__\n",
    "\n",
    "If we enter parameters __intuitively__ : max_feature=1500, stop_words='english', ngram_range=(1, 1)\n",
    "- cross val score: 0.872\n",
    "- train score    : 0.914\n",
    "- test score     : __0.524__\n",
    "\n",
    "**_So Grid Search is not the best solution always ????_**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters <br>\n",
    "{'vec__max_features': 5000,\n",
    " 'vec__ngram_range': (1, 2),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec=CountVectorizer(   max_features=250,\n",
    "                        ngram_range=(1, 1),\n",
    "                        stop_words=None,\n",
    "#                     tokenizer=None,\n",
    "#                     token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>advance</th>\n",
       "      <th>advice</th>\n",
       "      <th>after</th>\n",
       "      <th>airport</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>am</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>www</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  able  about  advance  advice  after  airport  all  also  am  ...   \\\n",
       "0   0     0      0        0       0      1        0    0     0   1  ...    \n",
       "1   0     0      0        0       1      0        0    0     0   0  ...    \n",
       "2   0     0      0        0       0      0        0    0     0   0  ...    \n",
       "3   0     0      0        3       0      1        0    0     0   0  ...    \n",
       "4   0     0      2        0       1      3        0    0     0   0  ...    \n",
       "\n",
       "   work  world  worth  would  www  x200b  year  years  you  your  \n",
       "0     0      0      0      0    0      0     0      0    1     0  \n",
       "1     0      0      0      1    0      0     0      0    3     1  \n",
       "2     0      0      0      0    0      0     0      0    0     0  \n",
       "3     0      0      0      0    0      0     0      0    0     0  \n",
       "4     0      0      0      1    0      0     0      0    2     3  \n",
       "\n",
       "[5 rows x 250 columns]"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec=pd.DataFrame(cvec.fit_transform(X_train['merged']).todense(), columns=cvec.get_feature_names())\n",
    "X_train_cvec.head()\n",
    "\n",
    "# what is the aim of the TODENSE(). if i dont write it then code doesnt work, why???????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 250)"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 250)"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cvec=pd.DataFrame(cvec.fit_transform(X_test['merged']).todense(),columns=cvec.get_feature_names())\n",
    "X_test_cvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>advance</th>\n",
       "      <th>advice</th>\n",
       "      <th>after</th>\n",
       "      <th>airport</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>am</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>www</th>\n",
       "      <th>x200b</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "      <td>1318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.059181</td>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.278452</td>\n",
       "      <td>0.043247</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>0.063733</td>\n",
       "      <td>0.225341</td>\n",
       "      <td>0.164643</td>\n",
       "      <td>0.235205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069044</td>\n",
       "      <td>0.073596</td>\n",
       "      <td>0.049317</td>\n",
       "      <td>0.386950</td>\n",
       "      <td>0.054628</td>\n",
       "      <td>0.207891</td>\n",
       "      <td>0.081942</td>\n",
       "      <td>0.052352</td>\n",
       "      <td>0.417299</td>\n",
       "      <td>0.104704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.322980</td>\n",
       "      <td>0.309965</td>\n",
       "      <td>0.670910</td>\n",
       "      <td>0.217906</td>\n",
       "      <td>0.428113</td>\n",
       "      <td>0.351812</td>\n",
       "      <td>0.392262</td>\n",
       "      <td>0.639187</td>\n",
       "      <td>0.526642</td>\n",
       "      <td>0.759194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360036</td>\n",
       "      <td>0.328200</td>\n",
       "      <td>0.296518</td>\n",
       "      <td>0.920803</td>\n",
       "      <td>0.541425</td>\n",
       "      <td>0.938924</td>\n",
       "      <td>0.388864</td>\n",
       "      <td>0.269125</td>\n",
       "      <td>1.082927</td>\n",
       "      <td>0.494119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                10         able        about      advance       advice  \\\n",
       "count  1318.000000  1318.000000  1318.000000  1318.000000  1318.000000   \n",
       "mean      0.059181     0.043247     0.278452     0.043247     0.112291   \n",
       "std       0.322980     0.309965     0.670910     0.217906     0.428113   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       5.000000     6.000000     6.000000     3.000000     5.000000   \n",
       "\n",
       "             after      airport          all         also           am  \\\n",
       "count  1318.000000  1318.000000  1318.000000  1318.000000  1318.000000   \n",
       "mean      0.072838     0.063733     0.225341     0.164643     0.235205   \n",
       "std       0.351812     0.392262     0.639187     0.526642     0.759194   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       5.000000     7.000000    11.000000     6.000000     9.000000   \n",
       "\n",
       "          ...              work        world        worth        would  \\\n",
       "count     ...       1318.000000  1318.000000  1318.000000  1318.000000   \n",
       "mean      ...          0.069044     0.073596     0.049317     0.386950   \n",
       "std       ...          0.360036     0.328200     0.296518     0.920803   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "max       ...          6.000000     4.000000     5.000000    13.000000   \n",
       "\n",
       "               www        x200b         year        years          you  \\\n",
       "count  1318.000000  1318.000000  1318.000000  1318.000000  1318.000000   \n",
       "mean      0.054628     0.207891     0.081942     0.052352     0.417299   \n",
       "std       0.541425     0.938924     0.388864     0.269125     1.082927   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max      13.000000    15.000000     6.000000     4.000000    19.000000   \n",
       "\n",
       "              your  \n",
       "count  1318.000000  \n",
       "mean      0.104704  \n",
       "std       0.494119  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       8.000000  \n",
       "\n",
       "[8 rows x 250 columns]"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i.count for i in X_train_cvec.columns if i!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MultinomialNB() # instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb=nb.fit(X_train_cvec,y_train) # fit the model to counterized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801037529321482"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_nb,X_train_cvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8846737481031867"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6892307692307692"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_test_cvec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[131, 190],\n",
       "       [ 12, 317]])"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,model_nb.predict(X_test_cvec))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6892"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round((cm[0][0]+cm[1][1])/cm.sum(),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b-Tf-IdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters <br>\n",
    "{'vec__max_features': 1000,\n",
    " 'vec__ngram_range': (1, 1),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvec=TfidfVectorizer(max_features=2000,\n",
    "                     ngram_range=(1,2),\n",
    "                     stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfvec=pd.DataFrame(tfvec.fit_transform(X_train['merged']).todense(),columns=tfvec.get_feature_names())\n",
    "X_test_tfvec=pd.DataFrame(tfvec.fit_transform(X_test['merged']).todense(),columns=tfvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 2000)"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 2000)"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975702353834473"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(nb,X_train_tfvec,y_train,cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb2=nb.fit(X_train_tfvec,y_train) # fit the model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9453717754172989"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb2.score(X_train_tfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb2.score(X_test_tfvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c-hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  nb.fit(X_train_hvec,y_train) # after running --- 'Input X must be non-negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc=svm.SVC(C=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a-countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters for kernel='rbf' <br>\n",
    "{'vec__max_features': 250,\n",
    " 'vec__ngram_range': (1, 1),\n",
    " 'vec__stop_words': 'english'} <br>\n",
    "- feeder: <br>\n",
    "{'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "         tokenizer=None, vocabulary=None),\n",
    " 'model': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "   tol=0.001, verbose=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters for kernel='linear' <br>\n",
    "{'vec__max_features': None,\n",
    " 'vec__ngram_range': (1, 1),\n",
    " 'vec__stop_words': 'english'} <br>\n",
    "- feeder :<BR>{'vec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
    "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
    "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "         tokenizer=None, vocabulary=None),\n",
    " 'model': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
    "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "   tol=0.001, verbose=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we need to convert data into number in order to apply SVM. So lets do countectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec=CountVectorizer(max_features=250, stop_words='english')\n",
    "X_train_cvec=cvec.fit_transform(X_train['merged'])\n",
    "X_test_cvec=cvec.fit_transform(X_test['merged'])\n",
    "\n",
    "# QUESTION TO INSTRUCTOR?\n",
    "# when i want to apply the output of the Grid search it returned me error for \n",
    "# max_features=None. i think the problem is that the model firstly fits to X_train\n",
    "# and model size is equal to X_train size(here is number of columns=9177 ). then i\n",
    "# try to apply the model, which has same nummber of columns with X_train, to X_test \n",
    "# and it fails because size of X_test(columns=5643) doesnt match with size of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature datas' format must be in array or matrix. if we wouldnt have matrix shape \n",
    "# so i would transform the shape into array as below code.\n",
    "# pd.DataFrame(X_train_cvec.toarray(),columns=cvec.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.fit(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8960387902266804"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_svc,X_train_cvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9468892261001517"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8369230769230769"
      ]
     },
     "execution_count": 619,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_test_cvec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[271,  50],\n",
       "       [ 56, 273]])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,model_svc.predict(X_test_cvec))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters for kernel='linear'<BR>\n",
    "{'vec__max_features': 5000,\n",
    " 'vec__ngram_range': (1, 1),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvec=TfidfVectorizer(max_features=245,\n",
    "                     ngram_range=(1,1),\n",
    "                     stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfvec=pd.DataFrame(tfvec.fit_transform(X_train['merged']).todense(),columns=tfvec.get_feature_names())\n",
    "X_test_tfvec=pd.DataFrame(tfvec.fit_transform(X_test['merged']).todense(),columns=tfvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 245)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 245)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.fit(X_train_tfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8975654638065859"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_svc,X_train_tfvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9317147192716236"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_train_tfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8015384615384615"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_test_tfvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c- hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec=HashingVectorizer(stop_words='english') # i used default value for n_features\n",
    "X_train_hvec=hvec.fit_transform(X_train['merged'])\n",
    "X_test_hvec=hvec.fit_transform(X_test['merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 1048576)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 1048576)"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_hvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.fit(X_train_hvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9173114240185049"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_svc,X_train_hvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9863429438543247"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_train_hvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9292307692307692"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_test_hvec,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[312,   9],\n",
       "       [ 37, 292]])"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,model_svc.predict(X_test_hvec))\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-LOGISTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a-countvectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters <br>\n",
    "{'vec__max_features': 5000,\n",
    " 'vec__ngram_range': (1, 2),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log=LogisticRegression(penalty='l2') # 1- instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- prepare the X_train for fitting to the model. \n",
    "# a- firstly convert X_train and X_test from text to number, that is use any vector\n",
    "# b- vector needs to fit and transfrom the text(so format of data must be series, matrix, array etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec=CountVectorizer(max_features=30,\n",
    "                    tokenizer=None,\n",
    "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
    "                    stop_words='english',\n",
    "                    ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_cvec=pd.DataFrame(cvec.fit_transform(X_train['merged']).todense(),\n",
    "                          columns=cvec.get_feature_names())\n",
    "X_test_cvec=pd.DataFrame(cvec.fit_transform(X_test['merged']).todense(),\n",
    "                          columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.fit(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8482450264898791"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_log,X_train_cvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858877086494689"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_train_cvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7984615384615384"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_test_cvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b- Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- best_paramaters <br>\n",
    "{'vec__max_features': 1000,\n",
    " 'vec__ngram_range': (1, 1),\n",
    " 'vec__stop_words': 'english'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvec=TfidfVectorizer(max_features=25,\n",
    "                     ngram_range=(1,1),\n",
    "                     stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfvec=pd.DataFrame(tfvec.fit_transform(X_train['merged']).todense(),\n",
    "                           columns=tfvec.get_feature_names())\n",
    "X_test_tfvec=pd.DataFrame(tfvec.fit_transform(X_test['merged']).todense(),\n",
    "                          columns=tfvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.fit(X_train_tfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8444456038975015"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_log,X_train_tfvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858877086494689"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_train_tfvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_test_tfvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c-hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec=HashingVectorizer(stop_words='english')\n",
    "X_train_hvec=hvec.fit_transform(X_train['merged'])\n",
    "X_test_hvec=hvec.fit_transform(X_test['merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 1048576)"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_hvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.fit(X_train_hvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8778195035946668"
      ]
     },
     "execution_count": 655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model_log,X_train_hvec,y_train,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9347496206373292"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_train_hvec,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9030769230769231"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_log.score(X_test_hvec,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT THE BEST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The _best score_ for test data is **SVM-hash**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts=model_svc.predict(X_test_hvec)\n",
    "predicts[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[312,   9],\n",
       "       [ 37, 292]])"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm=confusion_matrix(y_test,predicts)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict - 0</th>\n",
       "      <th>predict - 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual - 0</th>\n",
       "      <td>312</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual - 1</th>\n",
       "      <td>37</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict - 0  predict - 1\n",
       "actual - 0          312            9\n",
       "actual - 1           37          292"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cm, columns=['predict - 0','predict - 1'],\n",
    "             index=['actual - 0','actual - 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    329\n",
       "0    321\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
